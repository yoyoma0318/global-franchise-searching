#!/usr/bin/env python3
"""
News Collector for Global Franchise Searching
Collects news from Google News RSS feeds and stores in Firestore
"""

import feedparser
import firebase_admin
from firebase_admin import credentials, firestore
from datetime import datetime
from urllib.parse import quote
import os
import time
import hashlib
from dotenv import load_dotenv

# Load environment variables
load_dotenv('.env.local')

# Initialize Firebase Admin SDK
def initialize_firebase():
    """Initialize Firebase Admin SDK if not already initialized"""
    if not firebase_admin._apps:
        # Get credentials from environment or use default
        cred_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
        project_id = os.getenv('NEXT_PUBLIC_FIREBASE_PROJECT_ID') or os.getenv('FIREBASE_PROJECT_ID')

        if cred_path and os.path.exists(cred_path):
            print(f"âœ“ Firebase ì¸ì¦ íŒŒì¼ ë°œê²¬: {cred_path}")
            cred = credentials.Certificate(cred_path)
            firebase_admin.initialize_app(cred, {
                'projectId': project_id
            })
        else:
            print("âš ï¸  GOOGLE_APPLICATION_CREDENTIALS not set, using Application Default Credentials")
            if project_id:
                cred = credentials.ApplicationDefault()
                firebase_admin.initialize_app(cred, {
                    'projectId': project_id
                })
            else:
                raise ValueError("Firebase project ID not found. Set NEXT_PUBLIC_FIREBASE_PROJECT_ID in .env.local")

    return firestore.client()

# RSS Feed URLs for Google News
NEWS_TOPICS = [
    {
        'name': 'Restaurant Industry Asia',
        'url': 'https://news.google.com/rss/search?q=Restaurant+Industry+Asia&hl=en-US&gl=US&ceid=US:en'
    },
    {
        'name': 'F&B Franchise Expansion',
        'url': 'https://news.google.com/rss/search?q=F%26B+Franchise+Expansion&hl=en-US&gl=US&ceid=US:en'
    },
    {
        'name': 'Restaurant M&A Asia',
        'url': 'https://news.google.com/rss/search?q=Restaurant+M%26A+Asia&hl=en-US&gl=US&ceid=US:en'
    }
]

def generate_article_id(link):
    """Generate unique ID from article link"""
    return hashlib.md5(link.encode()).hexdigest()

def parse_published_date(date_string):
    """Parse published date from RSS feed"""
    try:
        from email.utils import parsedate_to_datetime
        return parsedate_to_datetime(date_string)
    except:
        return datetime.now()

def collect_news_from_topic(db, topic_name, rss_url):
    """Collect news from a specific RSS feed"""
    print(f"\nğŸ” ìˆ˜ì§‘ ì‹œì‘... í† í”½: '{topic_name}'")
    print(f"   RSS: {rss_url}")

    try:
        # Parse RSS feed
        feed = feedparser.parse(rss_url)

        if not feed.entries:
            print(f"   âš ï¸  ë‰´ìŠ¤ ì—†ìŒ")
            return 0

        collected_count = 0
        skipped_count = 0

        # Get reference to market_intel collection
        market_intel_ref = db.collection('market_intel')

        for entry in feed.entries:
            try:
                # Generate unique article ID
                article_id = generate_article_id(entry.link)

                # Check if article already exists
                doc_ref = market_intel_ref.document(article_id)
                doc = doc_ref.get()

                if doc.exists:
                    skipped_count += 1
                    continue

                # Parse published date
                published_at = parse_published_date(entry.published if hasattr(entry, 'published') else None)

                # Extract source from link (domain name)
                from urllib.parse import urlparse
                source = urlparse(entry.link).netloc.replace('www.', '').replace('news.google.com', 'Google News')

                # Get summary
                summary = entry.summary if hasattr(entry, 'summary') else entry.title[:200]

                # Prepare document data
                article_data = {
                    'title': entry.title,
                    'link': entry.link,
                    'source': source,
                    'published_at': published_at,
                    'summary': summary,
                    'topic': topic_name,
                    'collected_at': datetime.now(),
                    'type': 'news'
                }

                # Save to Firestore
                doc_ref.set(article_data)
                collected_count += 1

                print(f"   âœ… ë‰´ìŠ¤ [{entry.title[:60]}...] ì €ì¥ ì™„ë£Œ")

            except Exception as e:
                print(f"   âŒ ì—ëŸ¬ (í•­ëª© ì²˜ë¦¬): {str(e)}")
                continue

        print(f"   ğŸ“Š ì™„ë£Œ: {collected_count}ê°œ ìˆ˜ì§‘, {skipped_count}ê°œ ìŠ¤í‚µ")
        return collected_count

    except Exception as e:
        print(f"   âŒ ì—ëŸ¬ (í† í”½ '{topic_name}'): {str(e)}")
        return 0

def collect_all_news():
    """Collect news from all topics"""
    print("\n" + "=" * 70)
    print("ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì‹œì‘")
    print("=" * 70)

    # Initialize Firebase
    print("\nğŸ“¡ Firebase ì—°ê²° ì¤‘...")
    db = initialize_firebase()
    print("âœ“ Firebase ì—°ê²° ì™„ë£Œ\n")

    total_collected = 0

    for topic in NEWS_TOPICS:
        collected = collect_news_from_topic(db, topic['name'], topic['url'])
        total_collected += collected
        time.sleep(2)  # Rate limiting

    print("\n" + "=" * 70)
    print(f"âœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {total_collected}ê°œì˜ ë‰´ìŠ¤ê°€ market_intel ì»¬ë ‰ì…˜ì— ì €ì¥ë¨")
    print("=" * 70 + "\n")

    return total_collected

def main():
    """Main function"""
    try:
        collect_all_news()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
